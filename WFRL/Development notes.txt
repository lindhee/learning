
Att göra:
- Why doesn't W converge to something reasonable, and/or the game play to better scores? 
- Do we need to freeze the policy we use and just update it every N steps, to avoid weight oscillation?
- I would like to try adding some randomness to the control signal (or resulting state), so we excite more states.
- Understand the Q_MAX value. Maybe print Q_MAX for each of the 11 states?

Om 11-vektorspelet
=====================
De ändringar som verkade göra att det blev bra, var:
- Ändra steglängden för Adam-optimeringen till 1e-2.
- Beräkna Q_hat med vikter W som fryses i 1000 tidssteg, för att undvika oscillationer i vikterna.

Exempel på ganska bra värden, efter 30 000 iterationer:
W = 
[[-19.8  -14.8   -9.9   -5.96  -2.99  -1.    -0.    -1.    -2.99  -5.96   -9.9 ]
 [-14.8   -9.9   -5.96  -3.1   -1.     0.    -1.    -2.99  -5.96  -9.9  -10.1 ]
 [ -9.9   -5.96  -2.99  -1.    -0.    -1.    -3.    -5.96  -9.9  -14.8  -10.07]]
 
Idealt skulle man få ungefär (bortsett från avklingningen gamma och att varje episod har begränsad längd):
-20 -15 -10  -6  -3  -1   0  -1  -3   -6  -10
-15 -10  -6  -3  -1   0  -1  -3  -6  -10  -15
<...och sista raden är som första, fast spegelvänd>
 
(Men notera att element (3,11) borde vara ca -20, så i den kolumnen blir det fel rangordning mellan de två sämsta alternativen, vilket iofs inte påverkar policyn.)

Några fler experiment antyder att den viktigaste ändringen var att sätta rätt steglängd. Även om jag inte fryser W_hat så får man hyfsade värden när man ligger nånstans mellan 1e-1 och 1e-3.

Förbättring:
Jag provade att slumpa ett index mellan -4 och 14 och sen låta [-4,0] mappas till 0 osv. Det gör att spelet oftare startar i ytterkanterna, så att vi får fler erfarenheter av att vara där. Sen tränade jag 100 gånger (30 000 iterationer per gång, med steglängd 1e-2, se commit f5e0cfdab095b). Varje resulterande W sparade jag som en radvektor i resulting_weights_padding.txt.
Sen gick jag tillbaka till att slumpa i intervallet [0,10] och träna 100 ggr igen, med i övrigt oförändrade parametrar. Resultatet finns i resulting_weights_no_padding.txt.
När jag plottar standardavvikelsen för vikterna i båda fallen (se plotWeightVariations.m), ser man att vikterna för yttersta fallen får tydligt mindre variation när man ser till att samla extra erfarenhet av ytterfallen. Samtidigt innebär väl det att man kanske tränar nätverket på lite fel fördelning av spel?


RL-bakgrund
===============
Om systemet befinner sig i tillstånd s och väljer aktion a, så kommer det till tillståndet s'=t(s,a) och får en belöning r'(s,a). Vi är ute efter att maximera den avklingande totala framtida belöningen r(s_t,a*) + \lambda r(s_{t+1},a*) + \lambda^2 r(s_{t+2},a*)..., där \lambda ligger inom (0,1) och a* är den optimala aktionen (i varje steg).

Bör man använda Q-learning eller policy learning?

Q-learning betyder att vi försöker estimera en fkn Q*(s,a) så att den är den maximala totala framtida belöningen (inkl. avklingning) som vi kan uppnå genom att göra aktion a från tillstånd s (och alltså sedan välja optimalt därefter). Belöningen för att nu vara i tillstånd s ingår alltså inte i Q. Om vi vet Q*(s,a) så kan vi i varje tillstånd s välja a = argmax_a Q*(s,t(s,a)).

Om vi kör massor av drag i verkligheten eller en simulator, får vi massor av kombinationer (s,a,r',s') som vi kan lära oss av.
Om det finns ett ändligt antal aktioner a och tillstånd s, kan vi göra en tabell Q(s,a) och uppdatera den enligt Bellmans ekvation:
Q(s,a) := r' + \lambda max_{a'} Q(s',a').
Om vi kör detta massor av gånger, så går Q(s,a) mot Q*(s,a).
- Det kräver förstås att vi testar alla tillstånd och aktioner många gånger.
- Nature-artikeln påpekar två viktiga saker för att få konvergens: Dels bör man frysa vikterna som man använder för att beräkna Q i HL av Bellmans ekvation (för att undvika oscillation av vikterna), dels bör man samla ihop en bank av sampel (s,a,r',s') som man sedan drar slumpmässigt ifrån för att få sina minibatcher. Då får man bort korrelationen mellan sampel som ligger nära varandra i tiden.

Om s har väldigt hög dimension (t.ex. består av en bild med en massa pixlar), kan det vara opraktiskt att iterera Bellmans ekvation över en tabell med alla tillstånd. Då kan vi istället träna ett neuralt nätverk till att approximera Q(s,a). För varje drag får vi en tupel (s,a,r',s') och modifierar vikterna w för att 
Q(s,a,w) ska gå mot (det kända) värdet T = r' + \lambda max_{a'} Q(s',a',\bar w), där \bar w hålls konstant medan vi gör SGD på varje batch.
- Samla erfarenheter och dra sedan en slumpässig kombination av drag till varje batch.

