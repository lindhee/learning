
Att göra:
- Understand the Q_MAX value. Maybe print Q_MAX for each of the 11 states?

Börja på en enkel WF-version
==============================
Använd pygames surfarray-typ.

Tillstånd: 
Roboten (en bitmap av röda pixlar) befinner sig i (x,y,theta) ovanpå en vit bild där svarta pixlar bildar väggar.

I varje drag: 
- Spelaren får välja en aktion bland ~9 körvektorer (från vänster centrumsväng, via rakt framåt till höger centrumsväng).
- Vi uppdaterar robotens pose.
- Vi hittar alla svarta bakgrundspixlar som ligger inom R pixlar från främre högra hörnet. De blir "städade" och byter färg till gröna.
- Vi samplar bakgrundbilden inom en rektangel (större än roboten) kring robotpositionen.
- Vi ritar roboten i sin nya pose.
- Vi samplar samma rektangel igen, och hittar alla pixlar som bytt färg från svart/grön till röd (=kollision).
- Spelaren får +1 poäng för varje svart pixel som byter färg och -1 poäng för varje röd pixel som ligger ovanpå en svart eller grön pixel.
- Vi roterar den andra rektangeln -theta och samplar ner den. Sen gör vi en binär matris som visar världen i robotens koordinatsystem och där element med 1 betyder att det finns ett hinder i den punkten. Det blir indata till spelaren, tillsammans med poängen.

Använd pygame.transform på en Surface för att skala och rotera.
Använd Surface.blit för att rita den ovanpå en annan Surface. Man kan också ange att vissa delar av en Surface är transparenta.

Vi skulle kunna göra hindren (och ytterväggen) massiva för att få högre straff på att kollidera. Men då måste vi göra skillnad på kantpixlar (som ger poäng när man städar dem) och inre pixlar (som bara ger straff vid kollision), så att inte sidborsten tjänar på att gå in långt in i väggen.

Q-funktionen skulle då vara ett neuralt nätverk som, givet bilden s, ger en poäng åt varje körvektor. Vi borde kunna börja med ett enlagers nätverk, där vikterna för varje körvektor borde gå att tolka som en heatmap för vilka hinder som talar för eller emot den körvektorn.

Gör en ny version av dummy_game, som vi kan provköra mot med en testfkn.


Om 11-vektorspelet
=====================
De ändringar som verkade göra att det blev bra, var:
- Ändra steglängden för Adam-optimeringen till 1e-2.
- Beräkna Q_hat med vikter W som fryses i 1000 tidssteg, för att undvika oscillationer i vikterna.

Exempel på ganska bra värden, efter 30 000 iterationer:
W = 
[[-19.8  -14.8   -9.9   -5.96  -2.99  -1.    -0.    -1.    -2.99  -5.96   -9.9 ]
 [-14.8   -9.9   -5.96  -3.1   -1.     0.    -1.    -2.99  -5.96  -9.9  -10.1 ]
 [ -9.9   -5.96  -2.99  -1.    -0.    -1.    -3.    -5.96  -9.9  -14.8  -10.07]]
 
Idealt skulle man få ungefär (bortsett från avklingningen gamma och att varje episod har begränsad längd):
-20 -15 -10  -6  -3  -1   0  -1  -3   -6  -10
-15 -10  -6  -3  -1   0  -1  -3  -6  -10  -15
<...och sista raden är som första, fast spegelvänd>
 
(Men notera att element (3,11) borde vara ca -20, så i den kolumnen blir det fel rangordning mellan de två sämsta alternativen, vilket iofs inte påverkar policyn.)

Några fler experiment antyder att den viktigaste ändringen var att sätta rätt steglängd. Även om jag inte fryser W_hat så får man hyfsade värden när man ligger nånstans mellan 1e-1 och 1e-3.

Förbättring:
Jag provade att slumpa ett index mellan -4 och 14 och sen låta [-4,0] mappas till 0 osv. Det gör att spelet oftare startar i ytterkanterna, så att vi får fler erfarenheter av att vara där. Sen tränade jag 100 gånger (30 000 iterationer per gång, med steglängd 1e-2, se commit f5e0cfdab095b). Varje resulterande W sparade jag som en radvektor i resulting_weights_padding.txt.
Sen gick jag tillbaka till att slumpa i intervallet [0,10] och träna 100 ggr igen, med i övrigt oförändrade parametrar. Resultatet finns i resulting_weights_no_padding.txt.
När jag plottar standardavvikelsen för vikterna i båda fallen (se plotWeightVariations.m), ser man att vikterna för yttersta fallen får tydligt mindre variation när man ser till att samla extra erfarenhet av ytterfallen. Samtidigt innebär väl det att man kanske tränar nätverket på lite fel fördelning av spel?


RL-bakgrund
===============
Om systemet befinner sig i tillstånd s och väljer aktion a, så kommer det till tillståndet s'=t(s,a) och får en belöning r'(s,a). Vi är ute efter att maximera den avklingande totala framtida belöningen r(s_t,a*) + \lambda r(s_{t+1},a*) + \lambda^2 r(s_{t+2},a*)..., där \lambda ligger inom (0,1) och a* är den optimala aktionen (i varje steg).

Bör man använda Q-learning eller policy learning?

Q-learning betyder att vi försöker estimera en fkn Q*(s,a) så att den är den maximala totala framtida belöningen (inkl. avklingning) som vi kan uppnå genom att göra aktion a från tillstånd s (och alltså sedan välja optimalt därefter). Belöningen för att nu vara i tillstånd s ingår alltså inte i Q. Om vi vet Q*(s,a) så kan vi i varje tillstånd s välja a = argmax_a Q*(s,t(s,a)).

Om vi kör massor av drag i verkligheten eller en simulator, får vi massor av kombinationer (s,a,r',s') som vi kan lära oss av.
Om det finns ett ändligt antal aktioner a och tillstånd s, kan vi göra en tabell Q(s,a) och uppdatera den enligt Bellmans ekvation:
Q(s,a) := r' + \lambda max_{a'} Q(s',a').
Om vi kör detta massor av gånger, så går Q(s,a) mot Q*(s,a).
- Det kräver förstås att vi testar alla tillstånd och aktioner många gånger.
- Nature-artikeln påpekar två viktiga saker för att få konvergens: Dels bör man frysa vikterna som man använder för att beräkna Q i HL av Bellmans ekvation (för att undvika oscillation av vikterna), dels bör man samla ihop en bank av sampel (s,a,r',s') som man sedan drar slumpmässigt ifrån för att få sina minibatcher. Då får man bort korrelationen mellan sampel som ligger nära varandra i tiden.

Om s har väldigt hög dimension (t.ex. består av en bild med en massa pixlar), kan det vara opraktiskt att iterera Bellmans ekvation över en tabell med alla tillstånd. Då kan vi istället träna ett neuralt nätverk till att approximera Q(s,a). För varje drag får vi en tupel (s,a,r',s') och modifierar vikterna w för att 
Q(s,a,w) ska gå mot (det kända) värdet T = r' + \lambda max_{a'} Q(s',a',\bar w), där \bar w hålls konstant medan vi gör SGD på varje batch.
- Samla erfarenheter och dra sedan en slumpässig kombination av drag till varje batch.

